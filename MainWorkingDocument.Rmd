---
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---
# Loading Packages and Data
```{r}
# install.packages("tidyverse")
# install.packages("fpp3")
# install.packages("imputeTS")
#install.packages("fGarch")
#install.packages("fitdistrplus")
#install.packages("rugarch")

rm(list = ls())
library(tidyverse)
library(fpp3)
library(magrittr)
library(imputeTS)
library(fGarch)
library(rugarch)
library(fitdistrplus)
data <- read_delim("BaselMessungen.csv", delim = ";")
data %<>% select(-c("Wasserstand","Pegel")) %>% as_tsibble()
```

# First EDA
```{r}
# setting a nice plotting theme as default
theme_set(
    theme_classic() + 
        theme(
            axis.ticks.length = unit(-0.25, "cm"),
            axis.text.x = element_text(margin = unit(c(0.4,0,0,0), "cm")),
            axis.text.y = element_text(margin = unit(c(0,0.4,0,0), "cm")),
            axis.line = element_blank(),
            panel.grid.major.y = element_line(linetype = 2),
            plot.title = element_text(hjust = 0.5),
            text = element_text(family = "serif"),
            legend.justification = c("right", "top"),
            # legend.position = c(1, 1),
            legend.position = c(.98, .98),
            legend.background = element_rect(fill = NA, color = "black"),
            panel.border = element_rect(fill = NA, size = 1.25),
            strip.text = element_text(size = 12)
            # legend.margin = margin(6, 10, 6, 6)
            # legend.box.background = element_rect(colour = "black")

                        )
    
    )

```
## First Inspectional Plot
```{r}
ggplot(data)+
  geom_line(aes(x=Zeitstempel,y=Abflussmenge))
```

## Summary Statistics
```{r}
dim(data)
interval_pull(data$Zeitstempel)
summary(data)
```

## Identifying Missing Values
```{r}
has_gaps(data)
count_gaps(data)
scan_gaps(data)

sum(is.na(data$Abflussmenge))
sum(is.na(data$Abflussmenge))/length(data$Zeitstempel)

# checking time consistency of measurements
check1 <- data %>% as_tibble() %>% drop_na() %>%
  arrange(Zeitstempel) %>%
  mutate(diff = lag(Zeitstempel) - Zeitstempel, diff = as.factor(diff)) %>%
  group_by(diff) %>%
  summarise(counter = n())
check1
```

## Visualising Missing Values
```{r}
# augmenting all time slots with missing data
# all_measurements <- tibble(
#   Zeitstempel = 
#     seq(
#       as.POSIXct(min(data$Zeitstempel)), as.POSIXct(max(data$Zeitstempel)),
#       by = "5 min")
#   )
# data %<>% full_join(all_measurements, by = "Zeitstempel")

data %<>% fill_gaps(.full = TRUE) # alternative

#old plot (points on time line)
# missing_data_plot <- data %>%
#   filter(is.na(Abflussmenge)) %>%
#   full_join(data[1, ]) %>%
#   mutate(missing_data = ifelse(is.na(Abflussmenge), TRUE, FALSE), missing_data = as.factor(missing_data)) %>%
#   ggplot(aes(Zeitstempel, 0, col = missing_data)) +
#   geom_point(size = 0.5) +
#   scale_x_yearmonth() +
#   theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
#   labs(col = "Missing Value")
# missing_data_plot

missing_datapoints <- data %>%
  filter(is.na(Abflussmenge)) %>% pull(Zeitstempel)

data %>% ggplot(aes(Zeitstempel, Abflussmenge)) + geom_line() + geom_vline(xintercept = missing_datapoints, col = "grey")
```

## More Sophisticated Inspectional Plot (comment für anderen plot; data im falschen zustand für den komment; to be moved)
```{r}
data %>% gg_tsdisplay(Abflussmenge, plot_type='partial')
```
Notes:
- indicates high positive correlation with recent past (1-2 days)
- indicates medium to low negative correlation with little less recent past (7-9 days)
- indicates low positive correlation with distant past (33 days)


## Impute Missing Values
```{r, warning = FALSE}
ggplot_na_intervals(data)
ggplot_na_gapsize(data)

data_imputed <- na_kalman(data)

aux <- data %>%
  as_tibble() %>%
  mutate(origin = "unimputed", missing = ifelse(is.na(Abflussmenge), 1, 0)) %>%
  full_join(data_imputed %>% as_tibble(), by = "Zeitstempel")


aux %>% filter(missing == 1)
aux_dot_size <- 0.1
data %>% ggplot(aes(Zeitstempel, Abflussmenge)) + geom_point(size = aux_dot_size, color = "grey") + geom_point(data = aux %>% filter(missing == 1), aes(Zeitstempel, Abflussmenge.y), col = "black", size = aux_dot_size) 

data <- data_imputed
```

## Inspecton of Auto-Correlation Function (not right place)
```{r}
acf(data, lag.max = 100)
# acf(data, lag.max = 1000)
# acf(data, lag.max = 10000)

# testing what a lag of 0.0034 means
# test <- acf(data, lag.max = 10000)
# test$lag %>% head()
# 5/0.003472222
# 1440 * 5/60
```
Due to the high frequency of measurements (every five minutes) and very long time series (144'000 observations), one needs to set the lag.max very high to see the autocorrelation between observations for longer time periods.

The acf plot appears to have a very strong autoregressive part, where perturbations to some mean carry a lot of momentum through time and induce very high autocorrelations over time.

For that reason, we can look at partial autocorrelations.
```{r}
pacf(data, lag.max = 100)
pacf(data, lag.max = 1000)
pacf(data, lag.max = 10000)

```

The logic behind partial autocorrelations filters the correlation of some time span for the autocorrelations of shorter time spans, leaving behind 'autocorrelation unexplained by autocorrelation of smaller time spans'. Precisely that logic, restricts the impact the high momentum perturbations have on the time series.

The plot indicates the following:
\begin{itemize}
  \item A statistically very significant strong but quickly decreasing negative auto-correlation for the very recent past (past 5 to 20 minutes).
  \item A statistically very significant moderate and slowly increasing auto-correlation for the little less recent past (past 20 to 45 minutes) which fades out within 20 minutes (past 45 to 65 minutes).
  \item A statistically significant weak but somewhat consistent auto-correlation for the somewhat distant past (past 95 - 115 minutes)
\end{itemize}

a statistically very significant quickly decreasing negative auto-correlation for the very recent past (past 5 to 20 minutes). A statistically very significant slowly increasing auto-correlation for the little less recent past (20 to 45 minutes) which fades out within 20 minutes.


```{r}
# pacf <- pacf(data, lag.max = 100000, plot = F)
# save(pacf, file = "pacf100000.rda")
load("pacf100000.rda")
pacf_data <- tibble(
  lag = c(0:(length(pacf$acf)-1)),
  autocorr = pacf$acf,
  ) %>% 
  filter(lag!=0) %>% 
  mutate(autocorr = as.double(autocorr))
```
With a significance level so low ($\frac{1}{\sqrt{n_{\text{obs}}}}$), a lot of auto-correlations are statistically significant, even if they are unreasonably low (e.g. 0.003).
```{r}
pacf_data %>% arrange(desc(abs(autocorr)))# %>%  filter(autocorr > 0.09)
```
However, only lags 1 and 9 are larger than 0.1 in absolute terms.
**Consequences?** #missing
```{r}
# lag plot of white noise
WN = ts(rnorm(3000))
lag.plot(WN, lag = 10)
acf(WN)
```

# Decomposition
## Decomposition with `stats::decompose()`
```{r}
frequency(data)
decomposition_simple <- as.ts(data, frequency = 288) %>% decompose()
autoplot(decomposition_simple)
```

Remark: We suspect the extreme measurement closely after 2300 is due to error. There are ca. 20 missing values closely before five measurements at around 1600 after which the time series continues at around value 800. We suspect that to be a measurement error.


## Decomposition with STL (LOESS)
Since ``stats::decompose()` decomposes into a trend and a single seasonal component only we will now try a different more fine-grained decomposition into multiple seasonal components. While doing so we also forego a central but also limiting assumption of the previous decomposition, being that the seasonality must be consistent (i.e. not varying) with time. In other words, a singel seasonality pattern is assumed to fit the whole time series, disregarding more fine-grained seasonalities, that aggregate to a seasonality with component with differing patterns over time.
```{r}
# # testing decomposition without imputed data
# data %<>% arrange(Zeitstempel)
# which(data$Zeitstempel == as_datetime("2021-04-18 09:00:00"))
# data <- data[86233:149422, ]

# decomposition using default values of the STL function
decomposition_stl <- data %>%
  model(stl = STL(Abflussmenge)) %>%
  components() 
```

## Comparing the Decompositions by Trend (Visually)
```{r}
data %>% ggplot(aes(Zeitstempel, Abflussmenge)) +
  geom_line(colour = "grey") +
  geom_line(aes(y = decomposition_stl$trend), colour = "red") +
  geom_line(aes(y = decomposition_simple$trend), colour = "blue") +
  labs(
    y = "flow rate",
    x = "time",
    title = "Flow Rate of the Rhine in Basel") #missing (legend missing)
```

## Comparing the Decompositions by Trend (Numerically)
```{r}
decomposition_comparison <- tibble(remainder_posttrend_stl = 
                data$Abflussmenge-decomposition_stl$trend, 
              remainder_posttrend_simple =
                data$Abflussmenge-decomposition_simple$trend
              )
decomposition_comparison %>% summarise(
  remainder_posttrend_stl_mean = mean(remainder_posttrend_stl, na.rm = T),
  remainder_posttrend_simple_mean = mean(remainder_posttrend_simple, na.rm = T)
)

```
## Conclusion on the Comparison
One can see that filtering the time series for the stl-trend leaves a smaller less varyying time series. That can be interpreted as the stl-trend captures more than the simple-trend. On the flip-side, this comparatively worse-fitting stl-trend can be deemed less overfitting, which is alsoindicated by various erratic movements in the trend. We will thus, continue with the trend estimated by stl.

```{r}
autoplot(decomposition_stl)
```

# Modelling
## Filtering for Trend
```{r}
data %<>% 
  mutate(Abflussmenge_f_t = Abflussmenge - decomposition_stl$trend)

data
```
### Inspecting the Filtered TS
```{r}
data %>%
  gg_tsdisplay(Abflussmenge_f_t, plot_type='partial')
```

As visible by the plot (:::), the time series is not yet covariance stationary, a requirement for further application fo ARMA and ARIMA modelling, as the variance of the time series varies heavily throughout the time span of the time series.
We will therefore continue to filter out additional seasonality components estimated with the Loess method.
### Quick Check (maybe out)
```{r}
data %>% mutate(first_diff = Abflussmenge_f_t - lag(Abflussmenge_f_t)) %>% 
  gg_tsdisplay(first_diff, plot_type = "partial")
```

## Filtering for Seasonalities
```{r}
data %<>% 
  mutate(
    Abflussmenge_f_ts = Abflussmenge_f_t - decomposition_stl$season_hour - decomposition_stl$season_day - decomposition_stl$season_week)
```
### Inspecting the Filtered TS
```{r}
data %>%
  gg_tsdisplay(Abflussmenge_f_ts, plot_type='partial')
```
The time series looses a bit of variance, especially at the highly varying time sections. We will therefore, now take first differences to make the time series more stationary and reduce the extent of varying variance.
### Quick Check (maybe out)
```{r}
data %<>% mutate(Abflussmenge_f_ts_fd = Abflussmenge_f_ts - lag(Abflussmenge_f_ts)) 
data %>% 
  gg_tsdisplay(Abflussmenge_f_ts_fd, plot_type = "partial")
```
This results in a time series not fully covariance stationary, as variance does increase strongly at time.
Additionally, auto-correlation is rather widely spread for a given time difference as visible in the next plot:
```{r}
lag.plot(ts(data$Abflussmenge_f_ts_fd[-1]), lags = 4, )
```
For the first few lagged plots of the time series (filtered for trend and seasonality as well as first differences taken) auto-correlation appears to be centered around zero, even if not always exactly zero (necessary for covariance stationarity). This hints to further patterns to be extractable, as it is also shown in strong (partial) auto-correlations:
```{r}
# pacf <- pacf(data$Abflussmenge_f_ts_fd[-1], lag.max = 100000, plot = F)
# save(pacf, file = "pacf100000_f_ts_fd.rda")
load("pacf100000_f_ts_fd.rda")
pacf_data <- tibble(
  lag = c(0:(length(pacf$acf)-1)),
  autocorr = pacf$acf,
  ) %>% 
  filter(lag!=0) %>% 
  mutate(autocorr = as.double(autocorr))
pacf_data %>% arrange(desc(abs(autocorr))) %>% filter(autocorr>0.09)
```
One can now observe 14 comparatively high and significant partial autocorrelations above 0.09 which allows for further pattern extraction.

```{r}
# Remark: takes long time to run
# commented out because takes long time to run. load("fits.rda") gets the object.
# fits <- data %>%
#   model(
#     arima = ARIMA(Abflussmenge ~ pdq(1:5, 1:5, 1:5)),
#     stepwise = ARIMA(Abflussmenge, ic = "aicc"), # searches on its own here (stepwise)
#     search = ARIMA(Abflussmenge, ic = "aicc", stepwise=FALSE)
#     )
# save(fits, file = "fits.rda")

load("fits.rda")
fits_backup <- fits

# fits %>% glance(fits) %>% arrange(desc(BIC))
# fits %>% select(arima) %>% report()
fits %>% select(stepwise) %>% report()
# fits %>% select(search) %>% report()


fits %>% 
  select(stepwise) %>%
  gg_tsresiduals()

# partial auto correlation after filtering for filtering for arima ---------------------------------
fits %>% select(stepwise) %>% report()

pacf <- pacf(data$Abflussmenge_f_ts_fd[-1], lag.max = 100000, plot = F)
# save(pacf, file = "pacf100000_f_ts_fd.rda")
# load("pacf100000_f_ts_fd.rda")
pacf_data <- tibble(
  lag = c(0:(length(pacf$acf)-1)),
  autocorr = pacf$acf,
  ) %>% 
  filter(lag!=0) %>% 
  mutate(autocorr = as.double(autocorr))
pacf_data %>% arrange(desc(abs(autocorr))) %>% filter(autocorr>0.09)

```
The arima fit that is found via a stepwise search (#unfinished explain more how this solution is found to show we understand it), yields very significant parameter estimates. (#idea one could visualise the estimates with confidence interval (error bars) and give interpretations to the estimates (the estimated arima model))
However, there are two issues: a) there persist strong autocorrelations, b) the error is (as previously hinted to) heteroscedastic.
<!-- ################################## -->
## Checking Error Term (Heteroscedasticity) (again, should only be done after differencing)

```{r}
#checking stl decomp (not our decomp. Therefore we should delete this.)
#is heteroscedastic
remainder <- decomposition_stl$remainder 
acf(remainder)
```

<!-- ################################## -->

## Fitting a GARCH model
```{r}
View(data)
#since differencing resulst in a NA in the first row we need to remove it
data_GARCH <- data %>% filter(!is.na(Abflussmenge_f_ts_fd))
plot(acf(data_GARCH$Abflussmenge_f_ts_fd^2, type = "correlation", lag.max = 100)[1:100])
plot(acf(data_GARCH$Abflussmenge_f_ts_fd^2, type = "correlation", lag.max = 1000)[1:1000])
acf(abs(data_GARCH$Abflussmenge_f_ts_fd), type = "correlation", lag.max = 100)
acf(abs(data_GARCH$Abflussmenge_f_ts_fd), type = "correlation", lag.max = 1000)
#hyperbolic decay can be seen in the remainder therefore => heteroscedastic

# code runs super long calculates AIC and BIC for different GARCH and ARCH Models:

#starting with normal distribution
condDist <- "norm"
#  condDist <- "std"

# ARCH- Model (no dependance on past variance)(we can kick this out right?)
#alpha_max <- 7
#m <- matrix(0, ncol = 2,nrow = alpha_max)
#N <- length(data_GARCH$Abflussmenge_f_ts_fd)
#for (alpha in c(1:alpha_max)){
#  gfit <- NULL
#  f <- as.formula(sprintf("~ garch(%d, 0)",alpha))
#  gfit  <- garchFit(formula = f, data=data_GARCH$Abflussmenge_f_ts_fd, cond.dist=condDist)
#  m[alpha,1] <- (2*(alpha+2)      +  2*gfit@fit$value)/N #aic
#  m[alpha,2] <- (log(N)*(alpha+2) +  2*gfit@fit$value)/N #bic
#}
# save(m, file = "IC_table_ARCH.rda")
 load("IC_table_ARCH.rda")
m
max(m[,1])
max(m[,2])
#best arch is alpha = 1

#Garch-Model
#alpha_max <- 7
#beta_max <- 7
#m_garch <- array(0, dim=c(alpha_max,beta_max,2))
#N <- length(data_GARCH$Abflussmenge_f_ts_fd)
#for (alpha in c(1:alpha_max)){
#  for (beta in c(1:beta_max)) {
#      gfit <- NULL
#     f <- as.formula(sprintf("~ garch(%d, %d)",alpha, beta))
#      gfit  <- garchFit(formula = f, data=data_GARCH$Abflussmenge_f_ts_fd, cond.dist=condDist)
#      m_garch[alpha,beta,1] <- (2*(alpha+beta+2)      +  2*gfit@fit$value)/N #aic
#      m_garch[alpha,beta,2] <- (log(N)*(alpha+beta+2) +  2*gfit@fit$value)/N #bic
#  }
#
#}
#
#save(m_garch, file = "IC_table_GARCH.rda")
load("IC_table_GARCH.rda")
m_garch
max(m_garch[,,1])
max(m_garch[,,2])

#best GARCH-Model is alpha= 1, beta = 5:
best_gfit  <- garchFit(formula = ~ garch(1,5), data=data_GARCH$Abflussmenge_f_ts_fd, cond.dist=condDist)
forecast_GARCH <- predict(n.ahead= 8640,best_gfit)

#using ugarch from now on
model_ugarch<-ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1, 5)), 
                  distribution.model = "norm")

#with Rolling to compare performance
#best_ugfit <- ugarchfit(spec=model_ugarch, data = data_GARCH$Abflussmenge_f_ts_fd, out.sample = 10000)
#garch_forecast_check <- ugarchforecast(best_ugfit, data = bitcoin, n.ahead = 8640, n.roll =8640)
#save(best_ugfit, file = "best_ugfit.rda")
#save(garch_forecast_check, file = "garch_forecast_check.rda")
load("best_ugfit.rda")
load("garch_forecast_check 2.rda")
plot(garch_forecast_check,which="all")



#forecast beyond time horizon
#best_ugfit_f <- ugarchfit(spec=model_ugarch, data = data_GARCH$Abflussmenge_f_ts_fd)
#save(best_ugfit_f, file = "best_ugfit_f.rda")
#load("best_ugfit_f.rda")
garch_forecast <- ugarchforecast(best_ugfit_f, data = bitcoin, n.ahead = 8640, n.roll =0, out.sample = 0)
save(garch_forecast, file = "best_ugfit_f.rda")
load("best_ugfit_f.rda")


#sigma (the conditional standard deviation) 
plot(x=c(1:8640),y= sigma(garch_forecast))

plot(x= data$Zeitstempel[-1],y=data_GARCH$Abflussmenge_f_ts_fd,type="l")
plot(x=c(1:8640),y= fitted(garch_forecast))

#necessary?
hist(data_GARCH$Abflussmenge_f_ts_fd, 
	100, 
	prob = TRUE,
	col="gray", main = "Histogram")

lines( sort(data_GARCH$Abflussmenge_f_ts_fd), 
       dnorm(sort(data_GARCH$Abflussmenge_f_ts_fd), 
             mean = mean(data_GARCH$Abflussmenge_f_ts_fd), 
	            sd = sqrt(var(data_GARCH$Abflussmenge_f_ts_fd))),
	lwd = 2
	)

qqnorm(data_GARCH$Abflussmenge_f_ts_fd)
qqline(data_GARCH$Abflussmenge_f_ts_fd)

#find good distribution?
descdist(data_GARCH$Abflussmenge_f_ts_fd, discrete = FALSE, method="sample")
descdist(decomposition_stl$remainder, discrete = FALSE, method="sample")
#distributions won´t fit well

```

## Blind fitting on TS itself
```{r}
# Remark: takes long time to run
# commented out because takes long time to run. load("fits.rda") gets the object.
# fits <- data %>%
#   model(
#     arima = ARIMA(Abflussmenge ~ pdq(1:5, 1:5, 1:5)),
#     stepwise = ARIMA(Abflussmenge, ic = "aicc"), # searches on its own here (stepwise)
#     search = ARIMA(Abflussmenge, ic = "aicc", stepwise=FALSE)
#     )
# save(fits, file = "fits.rda")

load("fits.rda")
fits_backup <- fits

fits %>% glance(fits) %>% arrange(desc(BIC))
fits %>% select(arima) %>% report()
fits %>% select(stepwise) %>% report()
fits %>% select(search) %>% report()
# they all end up with the same coefficient estimates 

fits %>% forecast(h=80) %>% autoplot(data)
fits %>% 
  select(arima) %>%
  gg_tsresiduals()

```

#arima blindfit mit forecast für die nächsten 16000 einheiten
```{r}


Forecast_arima_blindFit <- data %>% model(ARIMA(Abflussmenge))%>% report()

#ist das trash?
 Forecast_arima_blindFit %>% forecast(h=20000) %>% autoplot(data)
 Forecast_arima_blindFit %>% forecast(h=8000) %>% autoplot(data)
 Forecast_arima_blindFit %>% forecast(h=1000) %>% autoplot(data)
 
#sau geil
r <- Forecast_arima_blindFit %>% generate(h=16000, times = 4)
b <- Forecast_arima_blindFit %>% generate(h=16000)
r%>% autoplot(.sim) + autolayer(data,Abflussmenge)
b%>% autoplot(.sim) + autolayer(data,Abflussmenge)

forecast_arima_values <- b[,c(2,4)]
colnames(forecast_arima_values) <- c("Zeitstempel", "Abflussmenge")
```







- - - - - - - - - -
# Raw AR, MA, ARMA and ARIMA Functions

```{r}
my_MA <- function(ts, q, thetas){
  # verify equal length of q and thetas
  if(q+1!=length(thetas)){errorCondition("Inputs to my_MA are inapproriate.")}
  # thetas need to contain 1 as first value 
  # as the current noise innovation is not scaled => multiplied with 1
  out <- rep(NA, length(ts))
  
  for(i in seq_along(ts)){
    if(i-q<1){
      out[i] <- NA
      
    }else{
      out[i] <- sum(thetas * ts[i:i-q])
  }
  }
  return(out)  
}
my_AR <- function(ts, q, phis){
  # verify equal length of q and thetas
  if(q!=length(phis)){errorCondition("Inputs to my_AR are inapproriate.")} #switched thetas to phis
  # thetas need to contain 1 as first value 
  # as the current noise innovation is not scaled => multiplied with 1
  out <- rep(NA, length(ts))
  
  for(i in seq_along(ts)){
    if(i-q<1){
      out[i] <- ts[i]
      
    }else{
      out[i] <- sum(phis * out[seq(i-q,i-1,1)]) + ts[i]
    }
    out
  }
  return(out)  
}


# manual testing code
ts <- c(1:10) #wird unten verwendet tsibble von aux-ts
# q <- 3
# thetas <- c(1, 0.8, 0.9, 0.4)
# phis <- c(0.8, 0.9, 0.4)
# usual testing
# my_MA(
#   c(1:10),
#   3,
#   c(1, 0.8, 0.9, 0.4))
set.seed(123)
aux_ts <- rnorm(100)
ar <- my_AR(
  aux_ts,
  1,
  c(0.6))
aux_ts <- tsibble(ts = ts, index = c(1:length(ts)))
aux_ts %>% model(
  AR1 = ARIMA(ts ~ pdq(1, 0, 0), )
)
plot(c(1:length(ar)), ar, type ="l")

```

# Raw Auto-Corr
```{r}
t = 0:300
y_stationary <- rnorm(length(t),mean=1,sd=1) # the stationary time series (ts)
plot(t, y_stationary, "l")

my_autocorr <- function(ts, q){
    aux <- tibble(counter = as.integer(), autocorr = as.double())
    for (i in seq_along(c(1:q))){
    table <-
        tibble(
            ts = ts,
            lagged_ts = lag(ts, i)) %>%
        drop_na() %>%
        summarise(
            autocorr = cov(ts, lagged_ts)/(sd(ts) * sd(lagged_ts))
            )
    out <- table %>% pull(autocorr)
    aux <- aux %>% complete(counter = i, autocorr = out)
    }
    return(aux)
}

test <- acf(y_stationary, 7)
test$acf[-1]
my_autocorr(y_stationary, 7) %>% arrange(counter)  %>% pull(autocorr)
((test$acf[-1] - my_autocorr(y_stationary, 7) %>% arrange(counter)  %>% pull(autocorr))/test$acf[-1]) %>% mean()

```
works fairly accurate — relative error 1%

#turning forecast into flow velocity
```{r}
# forecasted_flowRate <- data[,c(1:2)]
forecasted_flowRate <- forecast_arima_values


#average of 101 m^3/s is introduced to the old Rhine
forecasted_flowRate$Abflussmenge <- forecasted_flowRate$Abflussmenge - 101

#capacity of the Rheinseitenkanal is limited to 1400m^3/s
forecasted_flowRate$Abflussmenge <- ifelse(forecasted_flowRate$Abflussmenge >1400,1400,forecasted_flowRate$Abflussmenge)

ggplot(forecasted_flowRate)+
  geom_line(aes(x=Zeitstempel,y=Abflussmenge))

#convert flow rate to flow velocity in m/s
flow_velocity <- forecasted_flowRate %>% mutate(Fliessgeschwindigkeit = Abflussmenge/630)
flow_velocity <- flow_velocity[,c(1,3)]
ggplot(flow_velocity)+
  geom_line(aes(x=Zeitstempel,y=Fliessgeschwindigkeit))



#enter travel distance in m
traveldistance <- 170000

#shipspeed in m/s (eigenantrieb)
shipspeed <- 3.33

#are you travelling with (downstream) or against (upstream) the stream?
downstream <- TRUE

#earliest departure date
eDepDate <- as.POSIXct("2022-01-01 07:00", format= "%Y-%m-%d %H:%M")
  
#latest arrival date
lArrDate <- as.POSIXct("2022-01-17 15:00", format= "%Y-%m-%d %H:%M")

#since 5 min difference between measurements calc travelled distance with current value for now and next 4 mins
if(downstream==TRUE){
flow_velocity$Fliessgeschwindigkeit <- (flow_velocity$Fliessgeschwindigkeit+shipspeed)*5*60
} else{
  flow_velocity$Fliessgeschwindigkeit <- (flow_velocity$Fliessgeschwindigkeit-shipspeed)*5*60
}

ggplot(flow_velocity)+
  geom_line(aes(x=Zeitstempel,y=Fliessgeschwindigkeit))


summary <- data.frame(DepartureTime =as.POSIXct(character(), format= "%Y-%m-%d %H:%M"),
                      Duration= integer(),
                      ArrivalTime = as.POSIXct(character(), format= "%Y-%m-%d %H:%M"))

timeframe <- seq(eDepDate, lArrDate,by = "5 min")
for (depTime in timeframe) {
  ####depTime <- eDepDate
  start<- match(depTime,flow_velocity$Zeitstempel)
  i<- start
  
  repeat{
    if(i>length(flow_velocity$Fliessgeschwindigkeit)){
      break
    }
    
  distance_travelled <- sum(flow_velocity$Fliessgeschwindigkeit[start:i])
  i <- i + 1 
  if(distance_travelled >= traveldistance){
    summary %<>% add_row(DepartureTime = flow_velocity$Zeitstempel[start]
                        ,Duration= ((i-start)*5),
                        ArrivalTime =flow_velocity$Zeitstempel[i])
    break
    }
  }
}

ggplot(summary, aes(x=Duration, y = ..prop..))+
  geom_bar()

ggplot(summary, aes(x= DepartureTime ,y=Duration))+
  geom_line()

optimal_departure_times <- as.data.frame(summary) %>% dplyr::filter(Duration == min(Duration))

```


