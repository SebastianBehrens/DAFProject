---
output: html_document
editor_options:
  chunk_output_type: console
---
# Loading Packages and Data
```{r}
# install.packages("tidyverse")
# install.packages("fpp3")
# install.packages("imputeTS")
rm(list = ls())
library(tidyverse)
library(fpp3)
library(magrittr)
library(imputeTS)
data <- read_delim("BaselMessungen.csv", delim = ";")
data %<>% select(-c("Wasserstand","Pegel")) %>% as_tsibble()
```

# First EDA
```{r}
# setting a nice plotting theme as default
theme_set(
    theme_classic() + 
        theme(
            axis.ticks.length = unit(-0.25, "cm"),
            axis.text.x = element_text(margin = unit(c(0.4,0,0,0), "cm")),
            axis.text.y = element_text(margin = unit(c(0,0.4,0,0), "cm")),
            axis.line = element_blank(),
            panel.grid.major.y = element_line(linetype = 2),
            plot.title = element_text(hjust = 0.5),
            text = element_text(family = "serif"),
            legend.justification = c("right", "top"),
            # legend.position = c(1, 1),
            legend.position = c(.98, .98),
            legend.background = element_rect(fill = NA, color = "black"),
            panel.border = element_rect(fill = NA, size = 1.25),
            strip.text = element_text(size = 12)
            # legend.margin = margin(6, 10, 6, 6)
            # legend.box.background = element_rect(colour = "black")

                        )
    
    )

```
## First Inspectional Plot
```{r}
ggplot(data)+
  geom_line(aes(x=Zeitstempel,y=Abflussmenge))
```
## More Sophisticated Inspectional Plot
```{r}
data %>%
  gg_tsdisplay(Abflussmenge, plot_type='partial')
```
Notes:
- indicates high positive correlation with recent past (1-2 days)
- indicates medium to low negative correlation with little less recent past (7-9 days)
- indicates low positive correlation with distant past (33 days)


## Summary Statistics
```{r}
dim(data)
interval_pull(data$Zeitstempel)
summary(data)
```

## Identifying Missing Values
```{r}
has_gaps(data)
count_gaps(data)
scan_gaps(data)

sum(is.na(data$Abflussmenge))
sum(is.na(data$Abflussmenge))/length(data$Zeitstempel)

# checking time consistency of measurements
check1 <- data %>% as_tibble() %>% drop_na() %>%
  arrange(Zeitstempel) %>%
  mutate(diff = lag(Zeitstempel) - Zeitstempel, diff = as.factor(diff)) %>%
  group_by(diff) %>%
  summarise(counter = n())
check1
```

## Visualising Missing Values
```{r}
# augmenting all time slots with missing data
# all_measurements <- tibble(
#   Zeitstempel = 
#     seq(
#       as.POSIXct(min(data$Zeitstempel)), as.POSIXct(max(data$Zeitstempel)),
#       by = "5 min")
#   )
# data %<>% full_join(all_measurements, by = "Zeitstempel")

data %<>% fill_gaps(.full = TRUE) # alternative

#old plot (points on time line)
# missing_data_plot <- data %>%
#   filter(is.na(Abflussmenge)) %>%
#   full_join(data[1, ]) %>%
#   mutate(missing_data = ifelse(is.na(Abflussmenge), TRUE, FALSE), missing_data = as.factor(missing_data)) %>%
#   ggplot(aes(Zeitstempel, 0, col = missing_data)) +
#   geom_point(size = 0.5) +
#   scale_x_yearmonth() +
#   theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
#   labs(col = "Missing Value")
# missing_data_plot

missing_datapoints <- data %>%
  filter(is.na(Abflussmenge)) %>% pull(Zeitstempel)

data %>% ggplot(aes(Zeitstempel, Abflussmenge)) + geom_line() + geom_vline(xintercept = missing_datapoints, col = "grey")
```


## Impute Missing Values
```{r, warning = FALSE}
# data %<>% fill_gaps(.full = TRUE) # somewhat obsolete after augmentation above
ggplot_na_intervals(data)
ggplot_na_gapsize(data)

data_imputed <- na_kalman(data)

aux <- data %>%
  as_tibble() %>%
  mutate(origin = "unimputed", missing = ifelse(is.na(Abflussmenge), 1, 0)) %>%
  full_join(data_imputed %>% as_tibble(), by = "Zeitstempel")



aux_dot_size <- 0.1
data %>% ggplot(aes(Zeitstempel, Abflussmenge)) + geom_point(size = aux_dot_size, color = "grey") + geom_point(data = aux %>% filter(missing == 1), aes(Zeitstempel, Abflussmenge.y), col = "black", size = aux_dot_size) 

data <- data_imputed
```

## Inspecton of Auto-Correlation Function
```{r}
# acf(data, lag.max = 100)
# acf(data, lag.max = 1000)
acf(data, lag.max = 10000)

# testing what a lag of 0.0034 means
# test <- acf(data, lag.max = 10000)
# test$lag %>% head()
# 5/0.003472222
# 1440 * 5/60
```
Due to the high frequency of measurements (every five minutes) and very long time series (144'000 observations), one needs to set the lag.max very high to see the autocorrelation between observations for longer time periods.

The acf plot appears to have a very strong autoregressive part, where perturbations to some mean carry a lot of momentum through time and induce very high autocorrelations over time.

For that reason, we can look at partial autocorrelations.
```{r}
pacf(data, lag.max = 100)
pacf(data, lag.max = 1000)
pacf(data, lag.max = 10000)

```

The logic behind partial autocorrelations filters the correlation of some time span for the autocorrelations of shorter time spans, leaving behind 'autocorrelation unexplained by autocorrelation of smaller time spans'. Precisely that logic, restricts the impact the high momentum perturbations have on the time series.

The plot indicates the following:
\begin{itemize}
  \item A statistically very significant strong but quickly decreasing negative auto-correlation for the very recent past (past 5 to 20 minutes).
  \item A statistically very significant moderate and slowly increasing auto-correlation for the little less recent past (past 20 to 45 minutes) which fades out within 20 minutes (past 45 to 65 minutes).
  \item A statistically significant weak but somewhat consistent auto-correlation for the somewhat distant past (past 95 - 115 minutes)
\end{itemize}

a statistically very significant quickly decreasing negative auto-correlation for the very recent past (past 5 to 20 minutes). A statistically very significant slowly increasing auto-correlation for the little less recent past (20 to 45 minutes) which fades out within 20 minutes.


```{r}
# pacf <- pacf(data, lag.max = 100000, plot = F)
# save(pacf, file = "pacf100000.rda")
load("pacf100000.rda")
pacf_data <- tibble(
  lag = c(0:(length(pacf$acf)-1)),
  autocorr = pacf$acf,
  ) %>% 
  filter(lag!=0) %>% 
  mutate(autocorr = as.double(autocorr))
```
With a significance level so low ($\frac{1}{\sqrt{n_{\text{obs}}}}$), a lot of auto-correlations are statistically significant, even if they are unreasonably low (e.g. 0.003).
```{r}
pacf_data %>% arrange(desc(abs(autocorr)))# %>%  filter(autocorr > 0.09)
```
However, only lags 1 and 9 are larger than 0.1 in absolute terms.
**Consequences?**
```{r}
# lag plot of white noise
WN = ts(rnorm(300))
lag.plot(WN, lag = 10)
```






# Decomposition
## Decomposition with `stats::decompose()`
```{r}
frequency(data)
decomposition_simple <- as.ts(data, frequency = 288) %>% decompose()

plot(decomposition_simple)
```

## Decomposition with STL (LOESS)
Since ``stats::decompose()` decomposes into a trend and a single seasonal component only we will now try a different more fine-grained decomposition into multiple seasonal components. While doing so we also forego a central but also limiting assumption of the previous decomposition, being that the seasonality must be consistent (i.e. not varying) with time. In other words, a singel seasonality pattern is assumed to fit the whole time series, disregarding more fine-grained seasonalities, that aggregate to a seasonality with component with differing patterns over time.
```{r}
# # testing decomposition without imputed data
# data %<>% arrange(Zeitstempel)
# which(data$Zeitstempel == as_datetime("2021-04-18 09:00:00"))
# data <- data[86233:149422, ]

# decomposition using default values of the STL function
decomposition_stl <- data %>%
  model(stl = STL(Abflussmenge)) %>%
  components() 
```

## Comparing the Decompositions by Trend (Visually)
```{r}
data %>% ggplot(aes(Zeitstempel, Abflussmenge)) +
  geom_line(colour = "grey") +
  geom_line(aes(y = decomposition_stl$trend), colour = "red") +
  geom_line(aes(y = decomposition_simple$trend), colour = "blue") +
  labs(
    y = "flow rate",
    x = "time",
    title = "Flow rate of the Rhine in Basel",)
```

## Comparing the Decompositions by Trend (Numerically)
```{r}

decomposition_comparison <- tibble(remainder_posttrend_stl = 
                data$Abflussmenge-decomposition_stl$trend, 
              remainder_posttrend_simple =
                data$Abflussmenge-decomposition_simple$trend
              )
decomposition_comparison %>% summarise(
  remainder_posttrend_stl_mean = mean(remainder_posttrend_stl, na.rm = T),
  remainder_posttrend_simple_mean = mean(remainder_posttrend_simple, na.rm = T)
)

```
## Conclusion on the Comparison
One can see that filtering the time series for the stl-trend leaves a smaller less varyying time series. That can be interpreted as the stl-trend captures more than the simple-trend. On the flip-side, this comparatively worse-fitting stl-trend can be deemed less overfitting, which is alsoindicated by various erratic movements in the trend. We will thus, continue with the trend estimated by stl.
```{r}
autoplot(decomposition)
```

## Checking Error Term (Heteroscedasticity)

```{r}
#is heteroscedastic
remainder <- decomposition$remainder
acf(remainder)
```

# Modelling
## Filtering for Trend
```{r}
data %<>% 
  mutate(Abflussmenge_f_t = Abflussmenge - decomposition_stl$trend)

data
```
### Inspecting the Filtered TS
```{r}
data %>%
  gg_tsdisplay(Abflussmenge_f_t, plot_type='partial')
```

As visible by the plot (:::), the time series is not yet covariance stationary, a requirement for further application fo ARMA and ARIMA modelling, as the variance of the time series varies heavily throughout the time span of the time series.
We will therefore continue to filter out additional seasonality components estimated with the Loess method.
### Quick Check (maybe out)
```{r}
data %>% mutate(first_diff = Abflussmenge_f_t - lag(Abflussmenge_f_t)) %>% 
  gg_tsdisplay(first_diff, plot_type = "partial")
```

## Filtering for Seasonalities
```{r}
data %<>% 
  mutate(
    Abflussmenge_f_ts = Abflussmenge_f_t - decomposition_stl$season_hour - decomposition_stl$season_day - decomposition_stl$season_week)
```
### Inspecting the Filtered TS
```{r}
data %>%
  gg_tsdisplay(Abflussmenge_f_ts, plot_type='partial')
```
The time series looses a bit of variance, especially at the highly varying time sections. We will therefore, now take first differences to make the time series more stationary and reduce the extent of varying variance.
### Quick Check (maybe out)
```{r}
data %<>% mutate(Abflussmenge_f_ts_fd = Abflussmenge_f_ts - lag(Abflussmenge_f_ts)) 
data %>% 
  gg_tsdisplay(Abflussmenge_f_ts_fd, plot_type = "partial")
```
This results in a time series not fully covariance stationary, as variance does increase strongly at time.
Additionally, auto-correlation is rather widely spread for a given time difference as visible in the next plot:
```{r}
lag.plot(ts(data$Abflussmenge_f_ts_fd[-1]), lags = 4, )
```
For the first few lagged plots of the time series (filtered for trend and seasonality as well as first differences taken) auto-correlation appears to be centered around zero, even if not always exactly zero (necessary for covariance stationarity). This hints to further patterns to be extractable, as it is also shown in strong (partial) auto-correlations:
```{r}
# pacf <- pacf(data$Abflussmenge_f_ts_fd[-1], lag.max = 100000, plot = F)
# save(pacf, file = "pacf100000_f_ts_fd.rda")
load("pacf100000_f_ts_fd.rda")
pacf_data <- tibble(
  lag = c(0:(length(pacf$acf)-1)),
  autocorr = pacf$acf,
  ) %>% 
  filter(lag!=0) %>% 
  mutate(autocorr = as.double(autocorr))
pacf_data %>% arrange(desc(abs(autocorr))) %>% filter(autocorr>0.09)
```
One can now observe 14 comparatively high and significant partial autocorrelations above 0.09 which allows for further pattern extraction.

```{r}
# Remark: takes long time to run
# commented out because takes long time to run. load("fits.rda") gets the object.
# fits <- data %>%
#   model(
#     arima = ARIMA(Abflussmenge ~ pdq(1:5, 1:5, 1:5)),
#     stepwise = ARIMA(Abflussmenge, ic = "aicc"), # searches on its own here (stepwise)
#     search = ARIMA(Abflussmenge, ic = "aicc", stepwise=FALSE)
#     )
# save(fits, file = "fits.rda")

load("fits.rda")
fits_backup <- fits

# fits %>% glance(fits) %>% arrange(desc(BIC))
# fits %>% select(arima) %>% report()
fits %>% select(stepwise) %>% report()
# fits %>% select(search) %>% report()


fits %>% 
  select(stepwise) %>%
  gg_tsresiduals()

# partial auto correlation after filtering for filtering for arima ---------------------------------
fits %>% select(stepwise) %>% report()

pacf <- pacf(data$Abflussmenge_f_ts_fd[-1], lag.max = 100000, plot = F)
# save(pacf, file = "pacf100000_f_ts_fd.rda")
# load("pacf100000_f_ts_fd.rda")
pacf_data <- tibble(
  lag = c(0:(length(pacf$acf)-1)),
  autocorr = pacf$acf,
  ) %>% 
  filter(lag!=0) %>% 
  mutate(autocorr = as.double(autocorr))
pacf_data %>% arrange(desc(abs(autocorr))) %>% filter(autocorr>0.09)

```
The arima fit that is found via a stepwise search (#unfinished explain more how this solution is found to show we understand it), yields very significant parameter estimates. (#idea one could visualise the estimates with confidence interval (error bars) and give interpretations to the estimates (the estimated arima model))
However, there are two issues: a) there persist strong autocorrelations, b) the error is (as previously hinted to) heteroscedastic.

## Fitting a GARCH model
## Blind fitting on TS itself
```{r}
# Remark: takes long time to run
# commented out because takes long time to run. load("fits.rda") gets the object.
# fits <- data %>%
#   model(
#     arima = ARIMA(Abflussmenge ~ pdq(1:5, 1:5, 1:5)),
#     stepwise = ARIMA(Abflussmenge, ic = "aicc"), # searches on its own here (stepwise)
#     search = ARIMA(Abflussmenge, ic = "aicc", stepwise=FALSE)
#     )
# save(fits, file = "fits.rda")

load("fits.rda")
fits_backup <- fits

fits %>% glance(fits) %>% arrange(desc(BIC))
fits %>% select(arima) %>% report()
fits %>% select(stepwise) %>% report()
fits %>% select(search) %>% report()
# they all end up with the same coefficient estimates 

fits %>% 
  select(arima) %>%
  gg_tsresiduals()
```








- - - - - - - - - -
# Raw AR, MA, ARMA and ARIMA Functions

```{r}
my_MA <- function(ts, q, thetas){
  # verify equal length of q and thetas
  if(q+1!=length(thetas)){errorCondition("Inputs to my_MA are inapproriate.")}
  # thetas need to contain 1 as first value 
  # as the current noise innovation is not scaled => multiplied with 1
  out <- rep(NA, length(ts))
  
  for(i in seq_along(ts)){
    if(i-q<1){
      out[i] <- NA
      
    }else{
      out[i] <- sum(thetas * ts[i:i-q])
  }
  }
  return(out)  
}
my_AR <- function(ts, q, phis){
  # verify equal length of q and thetas
  if(q!=length(thetas)){errorCondition("Inputs to my_AR are inapproriate.")}
  # thetas need to contain 1 as first value 
  # as the current noise innovation is not scaled => multiplied with 1
  out <- rep(NA, length(ts))
  
  for(i in seq_along(ts)){
    if(i-q<1){
      out[i] <- ts[i]
      
    }else{
      out[i] <- sum(phis * out[seq(i-q,i-1,1)]) + ts[i]
    }
    out
  }
  return(out)  
}


# manual testing code
# ts <- c(1:10)
# q <- 3
# thetas <- c(1, 0.8, 0.9, 0.4)
# phis <- c(0.8, 0.9, 0.4)
# usual testing
# my_MA(
#   c(1:10),
#   3,
#   c(1, 0.8, 0.9, 0.4))
set.seed(123)
aux_ts <- rnorm(100)
ar <- my_AR(
  aux_ts,
  1,
  c(0.6))
aux_ts <- tsibble(ts = ts, index = c(1:length(ts)))
aux_ts %>% model(
  AR1 = ARIMA(ts ~ pdq(1, 0, 0), )
)
plot(c(1:length(ar)), ar, type ="l")

```

# Raw Auto-Corr
```{r}
t = 0:300
y_stationary <- rnorm(length(t),mean=1,sd=1) # the stationary time series (ts)
plot(t, y_stationary, "l")

my_autocorr <- function(ts, q){
    aux <- tibble(counter = as.integer(), autocorr = as.double())
    for (i in seq_along(c(1:q))){
    table <-
        tibble(
            ts = ts,
            lagged_ts = lag(ts, i)) %>%
        drop_na() %>%
        summarise(
            autocorr = cov(ts, lagged_ts)/(sd(ts) * sd(lagged_ts))
            )
    out <- table %>% pull(autocorr)
    aux <- aux %>% complete(counter = i, autocorr = out)
    }
    return(aux)
}

test <- acf(y_stationary, 7)
test$acf[-1]
my_autocorr(y_stationary, 7) %>% arrange(counter)  %>% pull(autocorr)
((test$acf[-1] - my_autocorr(y_stationary, 7) %>% arrange(counter)  %>% pull(autocorr))/test$acf[-1]) %>% mean()

```
works fairly accurate — relative error 1%



