---
output: html_document
editor_options:
  chunk_output_type: console
---
# Loading Packages and Data
```{r}
# install.packages("tidyverse")
# install.packages("fpp3")
# install.packages("imputeTS")
rm(list = ls())
library(tidyverse)
library(fpp3)
library(magrittr)
library(imputeTS)
data <- read_delim("BaselMessungen.csv", delim = ";")
data %<>% select(-c("Wasserstand","Pegel")) %>% as_tsibble()
```

# First EDA
```{r}
# setting a nice plotting theme as default
theme_set(
    theme_classic() + 
        theme(
            axis.ticks.length = unit(-0.25, "cm"),
            axis.text.x = element_text(margin = unit(c(0.4,0,0,0), "cm")),
            axis.text.y = element_text(margin = unit(c(0,0.4,0,0), "cm")),
            axis.line = element_blank(),
            panel.grid.major.y = element_line(linetype = 2),
            plot.title = element_text(hjust = 0.5),
            text = element_text(family = "serif"),
            legend.justification = c("right", "top"),
            # legend.position = c(1, 1),
            legend.position = c(.98, .98),
            legend.background = element_rect(fill = NA, color = "black"),
            panel.border = element_rect(fill = NA, size = 1.25),
            strip.text = element_text(size = 12)
            # legend.margin = margin(6, 10, 6, 6)
            # legend.box.background = element_rect(colour = "black")
            )
    
    )

```
## First Inspectional Plot
```{r}
ggplot(data)+
  geom_line(aes(x=Zeitstempel,y=Abflussmenge))
```
## More Sophisticated Inspectional Plot
```{r}
data %>%
  gg_tsdisplay(Abflussmenge, plot_type='partial')
```
Notes:
- indicates high positive correlation with recent past (1-2 days)
- indicates medium to low negative correlation with little less recent past (7-9 days)
- indicates low positive correlation with distant past (33 days)


## Summary Statistics
```{r}
dim(data)
interval_pull(data$Zeitstempel)
summary(data)
```

## Identifying Missing Values
```{r}
has_gaps(data)
count_gaps(data)
scan_gaps(data)

sum(is.na(data$Abflussmenge))
sum(is.na(data$Abflussmenge))/length(data$Zeitstempel)

# checking time consistency of measurements
check1 <- data %>% as_tibble() %>% drop_na() %>%
  arrange(Zeitstempel) %>%
  mutate(diff = lag(Zeitstempel) - Zeitstempel, diff = as.factor(diff)) %>%
  group_by(diff) %>%
  summarise(counter = n())
check1
```

## Visualising Missing Values
```{r}
# augmenting all time slots with missing data
# all_measurements <- tibble(
#   Zeitstempel = 
#     seq(
#       as.POSIXct(min(data$Zeitstempel)), as.POSIXct(max(data$Zeitstempel)),
#       by = "5 min")
#   )
# data %<>% full_join(all_measurements, by = "Zeitstempel")

data %<>% fill_gaps(.full = TRUE) # alternative

#old plot (points on time line)
# missing_data_plot <- data %>%
#   filter(is.na(Abflussmenge)) %>%
#   full_join(data[1, ]) %>%
#   mutate(missing_data = ifelse(is.na(Abflussmenge), TRUE, FALSE), missing_data = as.factor(missing_data)) %>%
#   ggplot(aes(Zeitstempel, 0, col = missing_data)) +
#   geom_point(size = 0.5) +
#   scale_x_yearmonth() +
#   theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
#   labs(col = "Missing Value")
# missing_data_plot

missing_datapoints <- data %>%
  filter(is.na(Abflussmenge)) %>% pull(Zeitstempel)

data %>% ggplot(aes(Zeitstempel, Abflussmenge)) + geom_line() + geom_vline(xintercept = missing_datapoints, col = "grey")
```


## Impute Missing Values
```{r}
# data %<>% fill_gaps(.full = TRUE) # somewhat obsolete after augmentation above
ggplot_na_intervals(data)
ggplot_na_gapsize(data)

data_imputed <- na_kalman(data)
data <- data_imputed


data_imputed %>% as_tibble()
aux <- data %>%
  as_tibble() %>%
  mutate(origin = "unimputed", missing = ifelse(is.na(Abflussmenge), 1, 0)) %>%
  full_join(data_imputed %>% as_tibble() %>% mutate(origin = "imputed"), by = "Zeitstempel")


aux %>% filter(missing == 1)
data[!complete.cases(data),]
aux %>% mutate(is_imputed = ifelse(origin == "imputed", 1, 0)) %>% ggplot(aes(Zeitstempel, Abflussmenge, color = "is_imputed")) + geom_point()

aux_dot_size <- 0.1
data %>% ggplot(aes(Zeitstempel, Abflussmenge)) + geom_point(size = aux_dot_size, color = "grey") + geom_point(data = aux %>% filter(missing == 1), aes(Zeitstempel, Abflussmenge.y), col = "black", size = aux_dot_size) 
```

## Inspecton of Auto-Correlation Function
```{r}
# acf(data, lag.max = 100)
# acf(data, lag.max = 1000)
acf(data, lag.max = 10000)
```
Due to the high frequency of measurements (every five minutes) and very long time series (144'000 observations), one needs to set the lag.max very high to see the autocorrelation between observations for longer time periods.

The acf plot appears to have a very strong autoregressive part, where perturbations to some mean carry a lot of momentum through time and induce very high autocorrelations over time.

For that reason, we can look at partial autocorrelations.
```{r}
pacf(data, lag.max = 30)
pacf(data, lag.max = 100)
pacf_data <- pacf(data, lag.max = 100000, plot = F)
```

The logic behind partial autocorrelations filters the correlation of some time span for the autocorrelations of shorter time spans, leaving behind 'autocorrelation unexplained by autocorrelation of smaller time spans'. Precisely that logic, restricts the impact the high momentum perturbations have on the time series.

The plot indicates the following:
\begin{itemize}
  \item A statistically very significant strong but quickly decreasing negative auto-correlation for the very recent past (past 5 to 20 minutes).
  \item A statistically very significant moderate and slowly increasing auto-correlation for the little less recent past (past 20 to 45 minutes) which fades out within 20 minutes (past 45 to 65 minutes).
  \item A statistically significant weak but somewhat consistent auto-correlation for the somewhat distant past (past 95 - 115 minutes)
\end{itemize}

a statistically very significant quickly decreasing negative auto-correlation for the very recent past (past 5 to 20 minutes). A statistically very significant slowly increasing auto-correlation for the little less recent past (20 to 45 minutes) which fades out within 20 minutes.

# Decomposition
## Decomposition with `stats::decompose()`
```{r}
frequency(data)
decomposition_simple <- as.ts(data, frequency = 288) %>% decompose()

plot(decomposition_simple)
```

## Decomposition with STL (LOESS)
Since ``stats::decompose()` decomposes into a trend and a single seasonal component only we will now try a different more fine-grained decomposition into multiple seasonal components. While doing so we also forego a central but also limiting assumption of the previous decomposition, being that the seasonality must be consistent (i.e. not varying) with time. In other words, a singel seasonality pattern is assumed to fit the whole time series, disregarding more fine-grained seasonalities, that aggregate to a seasonality with component with differing patterns over time.
```{r}
# # testing decomposition without imputed data
# data %<>% arrange(Zeitstempel)
# which(data$Zeitstempel == as_datetime("2021-04-18 09:00:00"))
# data <- data[86233:149422, ]

# decomposition using default values of the STL function
decomposition <- data %>%
  model(stl = STL(Abflussmenge)) %>%
  components() 
# SS <- decomposition %>% pull(remainder) %>% my_squarer() %>% sum()
# SS

my_squarer <- function(x){x^2} 
my_squarer <- Vectorize(my_squarer)
optimal_i_values <- tibble(trend_i = as.double(), 
                           season_i = as.double(),
                           sum = as.double())

trend_i_max <- 5
season_i_max <- 5

for(aux_trend_i in c(1:trend_i_max)){
  for(aux_season_i in c(1:season_i_max)){
    aux_decomposition_sum <- data %>%
      model(stl = STL(Abflussmenge ~ trend(window = aux_trend_i) + season(window = aux_season_i))) %>%
      components() %>% pull(remainder) %>% my_squarer() %>% sum()
    
    optimal_i_values %<>% complete(trend_i = aux_trend_i,
                                   season_i = aux_season_i,
                                   sum = aux_decomposition_sum)
    
  }
    
}
optimal_i_values

test_decomposition <- data %>%
  model(stl = STL(Abflussmenge ~ trend(window = 3) + season(window = 5))) %>%
  components() 
data %>% model(test_decomposition)

test_decomposition %>%
  as_tsibble() %>%
  autoplot(Abflussmenge, color = "grey") +
  geom_line(mapping= aes(y=trend), colour = "black") +
  labs(
    y = "flow rate",
    x = "time",
    title = "Flow rate of the Rhine in Basel")
```

## Visualising the Decomposition
```{r}
decomposition %>%
  as_tsibble() %>%
  autoplot(Abflussmenge, color = "grey") +
  geom_line(mapping= aes(y=trend), colour = "black") +
  labs(
    y = "flow rate",
    x = "time",
    title = "Flow rate of the Rhine in Basel")
```

```{r}
autoplot(decomposition)
```

## Checking Error Term (Heteroscedasticity)
```{r}
#is heteroscedastic
remainder <- decomposition$remainder
acf(remainder)
```

# Modelling 
```{r}
# Remark: takes long time to run
# commented out because takes long time to run. load("fits.rda") gets the object.
# fits <- data %>%
#   model(
#     arima = ARIMA(Abflussmenge ~ pdq(1:5, 1:5, 1:5)),
#     stepwise = ARIMA(Abflussmenge, ic = "aicc"), # searches on its own here (stepwise)
#     search = ARIMA(Abflussmenge, ic = "aicc", stepwise=FALSE)
#     )
# save(fits, file = "fits.rda")

load("fits.rda")
fits_backup <- fits

fits %>% glance(fits) %>% arrange(desc(BIC))
fits %>% select(arima) %>% report()
fits %>% select(stepwise) %>% report()
fits %>% select(search) %>% report()
# they all end up with the same coefficient estimates 

fits %>% 
  select(arima) %>%
  gg_tsresiduals()
```








- - - - - - - - - -
# Raw AR, MA, ARMA and ARIMA Functions

```{r}
my_MA <- function(ts, q, thetas){
  # verify equal length of q and thetas
  if(q+1!=length(thetas)){errorCondition("Inputs to my_MA are inapproriate.")}
  # thetas need to contain 1 as first value 
  # as the current noise innovation is not scaled => multiplied with 1
  out <- rep(NA, length(ts))
  
  for(i in seq_along(ts)){
    if(i-q<1){
      out[i] <- NA
      
    }else{
      out[i] <- sum(thetas * ts[i:i-q])
  }
  }
  return(out)  
}
my_AR <- function(ts, q, phis){
  # verify equal length of q and thetas
  if(q!=length(thetas)){errorCondition("Inputs to my_AR are inapproriate.")}
  # thetas need to contain 1 as first value 
  # as the current noise innovation is not scaled => multiplied with 1
  out <- rep(NA, length(ts))
  
  for(i in seq_along(ts)){
    if(i-q<1){
      out[i] <- ts[i]
      
    }else{
      out[i] <- sum(phis * out[seq(i-q,i-1,1)]) + ts[i]
    }
    out
  }
  return(out)  
}


# manual testing code
# ts <- c(1:10)
# q <- 3
# thetas <- c(1, 0.8, 0.9, 0.4)
# phis <- c(0.8, 0.9, 0.4)
# usual testing
# my_MA(
#   c(1:10),
#   3,
#   c(1, 0.8, 0.9, 0.4))
set.seed(123)
aux_ts <- rnorm(100)
ar <- my_AR(
  aux_ts,
  1,
  c(0.6))
aux_ts <- tsibble(ts = ts, index = c(1:length(ts)))
aux_ts %>% model(
  AR1 = ARIMA(ts ~ pdq(1, 0, 0), )
)
plot(c(1:length(ar)), ar, type ="l")

```

# Raw Auto-Corr
```{r}
t = 0:300
y_stationary <- rnorm(length(t),mean=1,sd=1) # the stationary time series (ts)
plot(t, y_stationary, "l")

my_autocorr <- function(ts, q){
    aux <- tibble(counter = as.integer(), autocorr = as.double())
    for (i in seq_along(c(1:q))){
    table <-
        tibble(
            ts = ts,
            lagged_ts = lag(ts, i)) %>%
        drop_na() %>%
        summarise(
            autocorr = cov(ts, lagged_ts)/(sd(ts) * sd(lagged_ts))
            )
    out <- table %>% pull(autocorr)
    aux <- aux %>% complete(counter = i, autocorr = out)
    }
    return(aux)
}

test <- acf(y_stationary, 7)
test$acf[-1]
my_autocorr(y_stationary, 7) %>% arrange(counter)  %>% pull(autocorr)
((test$acf[-1] - my_autocorr(y_stationary, 7) %>% arrange(counter)  %>% pull(autocorr))/test$acf[-1]) %>% mean()

```
works fairly accurate — relative error 1%



